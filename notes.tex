% LATEX TEMPLATE 
% adapted https://infinitedescent.xyz/latex/ by wade
% other minor adjustments cited below

\documentclass[11pt]{article}

% Edit the following to change the title, author name and date
\title{Reasoning with Data Notes}
\author{saffron\_}
\date{}

% \usepackage[T1]{fontenc}
% \usepackage[urw-garamond]{mathdesign}
% \let\circledS\undefined

% Packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}

% Page setup
\setlength{\parskip}{10pt}
\setlength{\parindent}{0pt}
\geometry{
    paper={letterpaper}, % Change to 'a4paper' for A4 size
    marginratio={1:1},
    margin={1.25in}
}

% custom for this document
\newcommand{\addsection}[1]{\addcontentsline{toc}{section}{#1}\section*{#1}}
\newcommand{\bb}[1]{\mathbb{#1}} 
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\PreIm}{PreIm}

\begin{document}
\maketitle
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Start of document body %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\addsection{Course Summary}
\begin{itemize}
  \item see \url{lecture1written.pdf}
\end{itemize}

\addsection{Exploratory Data Analysis}
EDA for 1-variable categorical data
\begin{itemize}
  \item population: complete set on interest (eg all US workers). can't be measured perfectly
  \item sample: subset of pop that can actually be obtained
  \item parameter: summary of population (eg average weight). also can't be measured
  \item statistic: estimation of parameter using sample.
  \item inference: specifying estimate of a parameter (ie gving estimate and measure of how far off it is)
  \item individuals/units/cases: objects described in dataset
  \item variable: column of spreadsheet, something measured
  \item quantitative: numerical (eg age) 
  \item categorical/qualitative: not numerical (eg ethnicity)
  \item frequency table: show how often each \emph{categorical} variable shows up
  \begin{itemize}
    \item relative freq. table: how percent often they show up, adding up to 1 or 100\%: best summary of a categorical variable
  \end{itemize}
    \item frequency/percent/relative (add to 1) frequency bar graphs: to visualize categorical data. v. similar to their freq table variants.
  \item pie graph: to visualize a \emph{single} categorical variable, with each slice being a category. 
  % \item distribution: representation of data that shows what values the variable takes and how often
\end{itemize}
\newpage
EDA for 1-variable quantitative data
\begin{itemize}
  \item grouped frequency/grouped relative frequency table: for one quantitative var. boring version of a histogram. 
  \item histogram: for one quantitative variable. visualizes a grouped (rel.) freq table.
  \item distribution of one quantitative var (anal. doesn't really hold up for qualitative)
  \begin{itemize}
    \item modality: how many `clusters'? (eg unimodal/bimodal/etc)
    \item symmetric/skewness? (skew right means tail is to right)
    \item center: mean ($\overline{x}$)
    \begin{itemize}
      \item median if heavy outliers. using mode is cursed but you do you
    \end{itemize}
    \item spread: standard deviation ($S$) (usually, $\frac{2}{3}$ of values are 1 stddev away from mean)
    \begin{itemize}
      \item interquartile range (IQR) if heavy outliers.
    \end{itemize}
    \item outliers?
  \end{itemize}
  \item fun facts: variance: square of std dev. easier in formulas because it removes the square root, but in real world, std dev is preferred because it has same units as the data
  \item box plot: other way to graph quantitative data (using min, first quartile, median, third quartile, max) 
  \begin{itemize}
    \item interquartile range (IQR): difference between quartiles, a measure of spread. is rough, not as useful as stddev.
    \item shows less data than histogram, so best for concise comparison of \emph{multiple} distributions (see section on 2-variable EDA)
  \end{itemize}
\end{itemize}

\newpage
EDA for 2-variable data
\begin{itemize}
  \item relationship/association: one variable can tell you about another
  \item explanatory variable: ``input'' variable, the x-axis
  \item response variable: ``output'' variable, the y-axis
  \item if we need analysis from explanatory $\rightarrow$ response
  \begin{itemize}
    \item categorical $\rightarrow$ quantitative: side by side boxplots
    \begin{itemize}
      \item more difference if boxes are futher apart on ``y-axis''
      \item summaries: numerical summaries of response for each category
    \end{itemize}
    \item categorical $\rightarrow$ categorical: contingency table
    \begin{itemize}
      \item summaries: conditional percents of responses, conditioned on explanatory (ie what percent of people from Cali are stat majors?)
    \end{itemize}
    \item quantitative $\rightarrow$ quantitative: scatterplots
    \begin{itemize}
      \item describe: direction, form, outliers
      \item summaries (only if reasonably linear): correlation coefficient ($R$), least squares regression line ($\hat{y} = b_0 + b_1X$)
    \end{itemize}
    \item quantitative $\rightarrow$ categorical: outside scope of this course 
  \end{itemize}
\end{itemize}

\newpage
\addsection{Study Design}
\begin{itemize}
  \item no matter what, we want to consider and get\dots
  \begin{itemize}
    \item reliability, statistical significance: low random variation/error
    \begin{itemize}
      \item use a large sample size
      \item can be measured with stddev
    \end{itemize}
    \item validity: trustworthy estimates and predictions
    \begin{itemize}
      \item consider and declare outliers. remove them if needed, but with caution!
      \item beware extrapolation outside range of data that produced the model
      \item validate model (eg check for linearity if you use linear regression)
    \end{itemize} 
    \item generalizability: no bias aka systemic error 
    % (use random sampling) TODO merge with below
    \begin{itemize}
      \item called instrument bias if instrument is set wrong (can be social instrument such as misleading survey)
      \begin{itemize}
        \item remove via resetting instrument (reset scale, rewrite survey)
      \end{itemize}
      \item called sampling bias if sample is systematically not representative 
      \begin{itemize}
        \item remove via random sampling. such as simple random sampling (SRS): every individual has same chance to be chosen as any other; every pair same chance as other pair; etc
      \end{itemize}
    \end{itemize}
  \end{itemize}
  \item and if we have 2 or more variable relationships\dots
  \begin{itemize}
    \item causality: no lurking/confounding variables; if we want to find causation and not just correlation 
    \begin{itemize}
        \item use randomized assignment of explanatory variable
        \item experimental study: study where lurking/confounding variables are removed
        \item observational study: not experimental. subjects decided which treatments to get (eg survey vitamin c usage vs flu symptoms)
        \item placebo control group: `non-active' treatment to avoid placebo effect
        \item `double blind': neither the researchers nor the subjects know which treatment they are receiving
    \end{itemize}
  \end{itemize}
\end{itemize}
good luck on exam 1 $<$3

\newpage
\addsection{Elementary Probability}
\begin{itemize}
  \item probablility: measure of variation due to a random phenomenon
  \item random: produced so that probability applies. satisfies:
  \begin{itemize}
    \item equal likelihood ($A$'s selection is as likely as $B$'s)
    \item independence ($A$'s selection doesn't influence $B$'s)
  \end{itemize}
  \item coin: P(heads)$ = 0.5$
  \item relative frequency: if selections are random (eq likelihood and independent), 
  \[ \text{P(outcome)} = \frac{\text{\# of ways outcome occurs}}{\text{\# of possible outcomes}} \]
  \begin{itemize}
    \item long-run approx. can be as precise as desired: Law of Large Numbers
    \item not replacing violates independence
    \begin{itemize}
      \item consider a bag with a blue and red ball. selecting blue and not putting it back makes P(red) $ = 1$.
      \item but this is common---a pollster doesn't survey two houses twice
    \end{itemize}
    \item not replacing is fine if population $\ge$ 10 or 20 times the sample
    \begin{itemize}
      \item consider 2000 blue, 2000 red. selecting blue makes P(red) $\approx 0.5$
    \end{itemize}
  \end{itemize}
  \item probablility experiment: an outcome's success/failure varies
  \begin{itemize}
    \item eg flipping a coin: outcome (like get heads) varies
    \item trials: different ``runs'' of a probability experiment
  \end{itemize}
  \item disjoint/mutually exclusive: $A \cap B$ is empty
  \item elementary outcomes: irreducible for assigning probabilities. mutually disjoint.
  \item sample space: $S$, the set of all elementary outcomes in a probability experiment
  \item complement: $A^C$, ``opposite'' of a probability. $A + A^C = 1$.
  \item intersection: $\cap$, ``and'' of two probabilities. middle of venn diagram.
  \item union: $\cup$, ``or'' of two probabilities. all circles' overlap area.
  \begin{itemize}
    \item P$(A \cup B) = \text{P}(A) + \text{P}(B) - \text{P}(A \cap B)$
  \end{itemize}
  \item joint probability tables
  \begin{itemize}
    \item each inner box is an elementary outcome (and a joint probability like $A\cap B$)
    \item rows sum to row marginal probabilities, same with columns
    \item 2x2 tables can represent venn diagrams
  \end{itemize}
  \newpage
  \item conditional probability ($A$, if we know that $B$):
  \begin{itemize}
    \item P$(A \text{ given } B) = \text{P}(A \mid B) = \text{P}(A \cap B) \div \text{P}(B)$
    \item for a d20, P($4 \mid \text{even}$) $= 0.1$
    \item take from box and margin of a joint probability table
  \end{itemize}
  \item statistically independent: unassociated events. if you know the outcome of one event, it doesn't change the probability of the other.
  \begin{itemize}
    \item with variables: association b/t $A$ and $B$ is nearly what is expected from chance
    \item with sampling: any person similarly likely to be selected, regardless of previous selection
    \item $A$ and $B$ are independent
    \begin{itemize}
      \item iff $\text{P}(A\mid B) = \text{P}(A)$
      \item iff $\text{P}(A\mid B) = \text{P}(A\mid B^C)$
      \item iff $\text{P}(A \cap B) = \text{P}(A) \cdot \text{P}(B)$
    \end{itemize}
  \end{itemize}
\end{itemize}
\addsection{Probability Models of Data}
\begin{itemize}
  \item random variable: outcome of randomness that takes on numerical values
  \item discrete rv: takes on ``separated'' outcomes. modeled with table or rel-freq histogram
  \item continuous rv: takes on an interval of outcomes. modeled with density curve
  \item rv example: cars occupancy
  \begin{enumerate}
    \item define prob. experiment: randomly select car on road
    \item define rv:  let $X = $ number of people in car
    \item assign probabilites to $X$. denote prob. of $X=1$ as P($X=1$)
  \end{enumerate}
  \item Binomial Counts ``Experiment''/Study/Trial
  \begin{itemize}
    \item conditions:
    \begin{enumerate}[(a)]
      \item fixed sample size $n$ for every run of study
      \item two categories (success/failure) for each observation
      \item observations must succeed or fail independantly
      \item fixed prob $p$ of success for each observation
    \end{enumerate}
    \item conditions c and d are guaranteed if selections are random 
    \item the count of successes in $n$ trials is a discrete rv $X$, and follows the Binomial Distribution (ie, $X$ is binomial. see by graphing the histogram)
    \item P($X = x$) = $_nC_x \cdot p^x \cdot (1-p)^{n-x}$ 
    \item binomial coefficient $_nC_k = C^n_k = \binom{n}{k} = \frac{n!}{x!(n-x)!}$ (recall $0! = 1$)
    \item eg: 8 rand selected lab animals get vaccine w/ prevention rate of 40\% and then are infected. Then $X =$ surviving animals out of $n = 8$ with $p = 0.4$.
  \end{itemize}
  \newpage
  \item distributions (density curves)
  \begin{itemize}
    \item graph a histogram, imagine a idealized curve fitting it
    \begin{itemize}
      \item idealized curve means true populations, so variables are greek letters
      \item mean $= \mu$ (mu)
      \item stddev $= \sigma$ (sigma)
    \end{itemize}
    \item area = proportion of observations = relative freq = probability
    \item total area = 1
    \item exponential distribution
    \begin{itemize}
      \item let $X$ be the time until the next event, with events occuring randomly with a mean waiting time $\lambda$.
      \item denoted $X \sim \text{Exp}(\lambda)$
      \item fun fact: this is graphed $f(x) = \lambda e^{-\lambda x}$
      \item eg: time until next earthquake, time until a phone call ends
    \end{itemize}
    \item uniform distribution
    \begin{itemize}
      \item a process produces values $\in [a,b]$ with equal likelihood.
      \item denoted $X \sim \text{Uniform}(a,b)$
      \item fun fact: this is graphed $f(x) = \frac{1}{b-a}$ for $x\in [a,b]$, $f(x) = 0$ elsewhere
      \item eg: result of dice roll
    \end{itemize}
    \item normal/gaussian distribution
    \begin{itemize}
      \item models result of mixing many independant factors and Central Lim Thrm
      \item denoted $X \sim \text{N}(\mu = u, \sigma = s)$
      \item fun fact: these are variations on the graph $f(x) = e^{-x^2}$
      \item eg: human height
      \item points of inflection are 1 stddev away from mean
      \item 68/95/99.7 percent of population falls within 1/2/3 stddev of mean
    \end{itemize}
    \item standard normal distribution
    \begin{itemize}
      \item for discussing probability on normal distributions: on all normal curves, portions with the same $Z$ score have the same area.
      \item standardized/$Z$ score: stddevs from mean for a number
      \begin{itemize}
        \item $Z = \frac{\text{observation} - \mu}{\sigma}$
      \end{itemize}
      \item $\text{N}(\mu=0, \sigma=1)$ is already standardized. we may denote $Z \sim \text{N}(0,1)$
      \item eg: consider adult male weight $X$.
      \begin{itemize}
        \item we know $X \sim \text{N}(\mu=165, \sigma=30)$
        \item if P weighs 202 pounds, how unusual are they?
        \item $Z = \frac{202 - 165}{30} = 1.23$ and $\text{P}(X < 202) = \text{P}(Z<1.23)=0.8907$.
      \end{itemize}
      \item $Z$ score tables and such exist for these calculations
      \item be able to calculate: $Z$ score to prob, prob to $Z$ score, stdizing to find prob, unstdizing to find score (see Lecture 14 notes)
    \end{itemize}
  \end{itemize}
\end{itemize}
\addsection{Sampling Distribution of a Statistic}
\begin{itemize}
  \item population distribution: eg how long it takes for literally every lightbulb to burn out
  \item (sample) data distribution: eg how long it takes for this random sample of  $n=1000$ lightbulbs to burn out
  \item sampling distribution of a statistic: the probability distribution of all possible values of a statistic (from taking many samples of same sample size from same population)
  \item sampling distribution of $\overline{X}$, the mean
  \begin{itemize}
    \item center: if sampling is random, then mean of sample is the population mean. ie,
    \[ \mu_{\overline{x}} = \mu \text{ if sampling is random}\]
    \item spread: if sampling is random and outcomes are statistically independant (eg sampling with replacement or infinitely large pop), then 
    \[ \sigma_{\overline{x}}=\frac{\sigma}{\sqrt{n}} \]
    \item shape: if sample is large ($n\ge 30$), then by the Central Limit Theorem, the sampling distribution of $\overline{X}$ approaches the normal distribution. (resembles population otherwise)
    \item lightbulb example problem in Lecture 16. Note AFSOC for inference.
  \end{itemize}
  \item sampling distribution of $\hat{P}$, proportion
  \begin{itemize}
    \item consider a binomial study (in particular, random and independant) with sample size $n$ and success probability $p$. then,
    \item center: 
    \[ \mu_{\hat{p}} = p \]
    \item spread: 
    \[ \sigma_{\hat{p}} = \sqrt{\frac{p(1-p)}{n}} \]
    \item shape: if $np$ and $n(1-p)$ both $\ge 10$, then by the Central Limit Theorem, $\hat{P}$ approaches the normal distribution. (resembles population otherwise?? maybe?)
    \item coinflip example problem in Lecture 18.
  \end{itemize}
\end{itemize}
good luck on exam 2 $<$3

\newpage
\addsection{Formal Inference (Confidence Intervals, Hypo. Testing)}
\begin{itemize}
  \item we study two branches of formal inference:
  \begin{itemize}
    \item confidence intervals: what are most likely values of a parameter?
    \item hypothesis/significance testing: what is the likelihood of a parameter to be a value?
  \end{itemize}
  \item Confidence Intervals
  \begin{itemize}
    \item Theory: $\overline{x}$ is ``probably'' somewhat ``close'' to $\mu$.
    \begin{itemize}
      \item how probable? confidence level
      \item how close? margin of error
    \end{itemize}
    \item if random sampling, stat. independant, normal, then the confidence interval is
    \[ \overline{X} \pm Z_\text{critical} \frac{\sigma}{\sqrt{n}} \]
    \begin{itemize}
      \item where $n=$ sample size, $\overline{X}=$ sample mean, $\sigma=$ assumed population stdddev, $Z_\text{c}=1.645$ for $C=90\%$, $Z_\text{c}=2$ for $C=95\%$, $Z_\text{c}=3$ for $C=99.7\%$, etc
      \item eg: $\approx 95 \%$ of $\overline{x}$'s are within $\pm 2 \sigma_{\overline{x}}$
    \end{itemize}
    \item similarly, if binomial (in particular, random sampling and independent) and normal, the confidence interval is
    \[ \hat{p} \pm Z_\text{critical} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \]
  \end{itemize}
  \item Hypothesis Testing 
  \begin{enumerate}
    \item State Hypotheses
    \begin{itemize}
      \item null ($H_0$): $\mu$ = value
      \item alt ($H_a$): $\mu$ <, >, or $\ne$ value
    \end{itemize}
    \item choose $\alpha$ (usually $0.01$ or $0.05$)
    \item perform study, get sample statistic, get $p$ value of that sample statistic
    \item conclusion: if $p < \alpha$, this is very unlikely. we reject the null hypothesis
    \item state in context: what does parameter represent? what is the likely value/why?
  \end{enumerate}
  \item Note: `$\ne$' confidence test with significance $\alpha$ is equiv to making a confidence interval with level $1-\alpha$ ie $\alpha^C$
  \begin{itemize}
    \item ie: Hypothesis test says do not reject $H_0 \Leftrightarrow H_0$ value is inside confidence interval
  \end{itemize}
  \item TODO formal template for writing this. an example on lec24 p5 says ``since the p value is (not) less than 0.05, we do (not) reject the null hypo. [explain what this means].'' do NOT say ``accept the null hypo.''
  \newpage
  \item T-Test for mean when sigma unknown
  \begin{itemize}
    \item we instead use sample stddev, $S$.
    \item like how we call $\frac{\sigma}{\sqrt{n}}$ $\sigma_{\overline{x}}$, we call $\frac{S}{\sqrt{n}}$ standard error
    \item notice how similar our equations are
    \item hypothesis test for a mean
    \begin{itemize}
      \item test stat $= t = \frac{\overline{X}-\mu_{\text{null}}}{S / \sqrt{n}}$ (``stderrs away from mean'' instead of stddevs away from mean)
    \end{itemize}
    \item confidence interval for a mean
    \begin{itemize}
      \item $\overline{X} \pm t_{\text{critical}}\cdot \frac{S}{\sqrt{n}}$
      \item note: $t_{\text{critical}}$ is a bit over 2 for $95\%$ because the t distribution is fatter at tails than the normal
    \end{itemize}
    \item instead of $n\ge 30$ for normality, you may inspect the distribution. since t-distribution is robust, vauge normality is even fine as samples sizes increase (as long as no severe outliers). ie, if $n\ge 40$ or so, t dist is valid regardless of shape
    \item t approaches norm distribution as $n$ increases.
  \end{itemize}
  \item Use test for two independent means when wts relationship between \emph{two categorical} explantories and \emph{one quantitative} response.
  \begin{itemize}
    \item $H_0: \mu_1 - \mu_2 = 0$ and $H_A: \mu_1 - \mu_2 </>/\ne 0$
    \item estimator of difference: $\overline{x}_1 - \overline{x}_2$
    \item we can analyze $z$ or $t$ score for the sampling dist of the estimator (depending on if $\sigma$ known/$t$ dist valid)
  \end{itemize}
  \item inference for two proportions when wts relationship between \emph{two categorical} explantories and \emph{two categorical response}
  \begin{itemize}
    \item $H_0: p_1 - p_2 = 0$ and $H_A: p_1 - p_2 </>/\ne 0$
    \item estimator of difference: $\hat{p}_1 - \hat{p}_2$
    \item must analyze $z$ score. rmbr to check validity ($np, n(1-p) \ge 10$ for both explanatories)
  \end{itemize}
  \item one-way (one explanatory) ANOVA (ANalysis Of VAriance) test of means
  \begin{itemize}
    \item TODO group others accordingly like this, descr down here
    \item at least \emph{3 categorical} explanatory $\rightarrow$ \emph{one quantitative} response (like better test of two means)
    \item $H_0: \mu_1 = \mu_2 = \dots = \mu_k$ and $H_A:$ at least one $\mu$ is different
    \item we test $F$ score. reqs: random selection. independence of each population's mean (large pop for each). shape ($n\ge 30$ each). each population $\sigma$ is similar, all within factor of 2 is fine.
  \end{itemize}
  \item chi square ($\chi ^2$) test for contingency tables
  \begin{itemize}
    \item \emph{numerous} categorical $\rightarrow$ \emph{numerous} response (like better test of two proportions)
    \item $H_0:$ no association between cat. and resp. and $H_A:$ there is an association between cat. and resp.
    \item analyze $\chi^2$ score (fun fact: $\chi^2$ dist. is unimodal right skewed)
    \begin{itemize}
      \item expected cell = $\frac{\text{row total} \times \text{column total}}{\text{grand total}}$ (for the actual values, not percentages)
      \item $\chi^2 = \sum \frac{(\text{observed} - \text{expected})^2}{\text{expected}}$ for all cells
    \end{itemize}
    \item reqs: independent cells ie each individual must be in only one cell; random selection; independence (large pop); shape (expected count $\ge 5$, use observed count if expected is not reported by software)
    \item (what if shape not satisfied? in 36-200, group categories or remove one (be careful!). in later courses, use a distribution-free test)
  \end{itemize}
\end{itemize}
\begin{itemize}
  \item test of slope (linear regression)
  \begin{itemize}
    \item \emph{quantitative} categorial $\rightarrow$ \emph{quantitative} response
    \item population line: $Y=\beta_0+\beta_1 X$, sample line: $\hat{y}=b_0+b_1X$
    \item no relationship $\iff \beta_1 = 0.$ thus,
    \item $H_0: \beta_1=0$ and $H_\text{A}: \beta_1 < / > / \ne 0$
    \item fun fact: test score is $t=\frac{\text{slope estimate}-\text{null for slope}}{\text{stderr of slope}} = \frac{b_1-0}{SE_\text{slope}}$
    \item reqs: quant $\rightarrow$ quant, random, independence (no systemic deviation from line. check via look at scatterplot, or other tools), shape (large $n$), spread (a req that ANOVA also has) (check if vertical spread reasonably same at all $X$ values)
    \item nonlinear?
    \begin{itemize}
      \item right skewed? try $x^{1/2}, x^{1/3}, \dots, \ln(x)$. $\ln$ should be of strictly greater than 1.
      \item left skewed? try $x^2, x^3, \dots, e^x$
    \end{itemize}
  \end{itemize}
\end{itemize}
FYI: independence condition for many tests: if we alr know random sampling, is fulfilled by replacement or large pop compared to n. this is all that is needed.

good luck on the final <3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% End of document body   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
